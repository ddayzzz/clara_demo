{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Getting Started with Clara Train SDK\n",
    "Clara Train SDK consists of different modules as depicted below \n",
    "<br>![side_bar](screenShots/TrainBlock.png)\n",
    "\n",
    "By the end of this notebook you will:\n",
    "1. Understand the components of [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html)\n",
    "2. Know how to configure train config json to train a CNN\n",
    "3. Train a CNN with single and muiltple GPUs\n",
    "4. Fine tune a model\n",
    "5. Export a model \n",
    "6. Perform inference on test dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Prerequisites\n",
    "- Nvidia GPU with 8GB of memory (Pascal or newer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Resources\n",
    "It maybe helpful to watch the free GTC Digital 2020 talk covering the Clara Train SDK \n",
    "- [S22563](https://developer.nvidia.com/gtc/2020/video/S22563)\n",
    "Clara train Getting started: Core concepts, Bring Your Own Components (BYOC), AI assisted annotation (AIAA), AutoML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## DataSet \n",
    "This notebook uses a sample dataset (ie. a single image of spleen dataset) provided in the package to train a network for a few epochs. \n",
    "This single file is duplicated 32 times for the training set and 9 times for validation in order to mimic the full spleen dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "#### Disclaimer  \n",
    "In this Notebook we run sample training jobs for one or two epochs just to highlight the core concepts. \n",
    "A relatively small neural network is also used to ensure it runs on most GPUs.    \n",
    "For realistic training a user could increase the number of epochs, use larger neural networks and tune other parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Lets get started\n",
    "It is helpful to first check that we have an NVIDIA GPU available in the docker by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 18 15:32:17 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 35%   46C    P2    86W / 250W |  10972MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 37%   50C    P2    87W / 250W |  10972MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 33%   40C    P2    73W / 250W |  10972MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 34%   44C    P2    89W / 250W |  10972MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 29%   33C    P0    44W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 33%   34C    P0    69W / 250W |      0MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 34%   34C    P0    58W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 19%   32C    P0    51W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11638      C   python                          10969MiB |\n",
      "|    1   N/A  N/A     11638      C   python                          10969MiB |\n",
      "|    2   N/A  N/A     11638      C   python                          10969MiB |\n",
      "|    3   N/A  N/A     11638      C   python                          10969MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# following command should show all gpus available \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "The cell below defines a helper function that will be used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting MMAR_ROOT= brats_seg\n",
      "\u001b[0m\u001b[01;34mcommands\u001b[0m/  \u001b[34;42mconfig\u001b[0m/  \u001b[01;34mlogs\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  \u001b[34;42mresources\u001b[0m/  \u001b[01;34msubmit\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "MMAR_ROOT=\"brats_seg\"\n",
    "DOCKER_NAME='nvcr.io/nvidia/clara-train-sdk:v3.0'\n",
    "# 这个是物理机的数据集的路径\n",
    "DATASET_PREFIX='/home/liuyuan/shu_codes/datasets/brats'\n",
    "# RESNET 似乎不使用 pretrain\n",
    "PRETRAIN_PATH='/home/liuyuan/shu_codes/pretrain_models/for_clara'\n",
    "\n",
    "# EXP_NAME='segres_brats2018_2019_2020'\n",
    "print (\"setting MMAR_ROOT=\", MMAR_ROOT)\n",
    "%ls $MMAR_ROOT\n",
    "\n",
    "!mkdir -p $MMAR_ROOT/logs\n",
    "\n",
    "def printFile(filePath,lnSt,lnOffset):\n",
    "    print (\"showing \",str(lnOffset),\" lines from file \",filePath, \"starting at line\",str(lnSt))\n",
    "    lnOffset=lnSt+lnOffset\n",
    "    !< $filePath head -n \"$lnOffset\" | tail -n +\"$lnSt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Medical Model ARchive (MMAR)\n",
    "Clara Train SDK uses the [Medical Model ARchive (MMAR)](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/mmar.html). \n",
    "The MMAR defines a standard structure for organizing all artifacts produced during the model development life cycle. \n",
    "The Clara Train SDK basic idea is to get started on training deep learning models using intuitive configuration files as shown below:\n",
    "<br>![side_bar](screenShots/MMAR.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "You can download sample models for different problems from [NGC](https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&pageNumber=0&query=clara&quickFilter=&filters=) <br> \n",
    "All MMAR follow the structure provided in this Notebook. if you navigate to the parent folder structure it should contain the following subdirectories\n",
    "```\n",
    "./GettingStarted \n",
    "├── commands\n",
    "├── config\n",
    "├── docs\n",
    "├── eval\n",
    "├── models\n",
    "└── resources\n",
    "```\n",
    "\n",
    "* `commands` contains a number of ready-to-run scripts for:\n",
    "    - training\n",
    "    - training with multiple GPUS\n",
    "    - validation\n",
    "    - inference (testing)\n",
    "    - exporting models in TensorRT Inference Server format\n",
    "* `config` contains configuration files (in JSON format) for each training, \n",
    "validation, and deployment for [AI-assisted annotation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/aiaa/index.html) \n",
    "(_Note:_ these configuration files are used in the scripts under the `commands` folder)\n",
    "* `docs` contains local documentation for the model, but for a more complete view it is recommended that you visit the NGC model page\n",
    "* `eval` is used as the output directory for model evaluation (by default)\n",
    "* `models` is where the tensorflow checkpoint-formatted model is stored (`.index`, `.meta`, `.data-xxxxx-of-xxxxx`), and the corresponding graph definition files (`fzn.pb` for frozen models, and `trt.pb` for TRT models)\n",
    "* `resources` currently contains the logger configuration in the `log.config` file\n",
    "\n",
    "Some of the most important files you will need to understand to configure and use in Clara Train SDK are:\n",
    "\n",
    "1. `environment.json` which has important common parameters: \n",
    "    * `DATA_ROOT` is the root folder where the data with which we would like to train, validate, or test resides in\n",
    "    * `DATASET_JSON` expects the path to a JSON-formatted file \n",
    "    * `MMAR_CKPT_DIR` the path to the where the tensorflow checkpoint files reside\n",
    "    * `MMAR_EVAL_OUTPUT_PATH` the path to output evaluation metrics for the neural network during training, validation, and inference\n",
    "    * `PROCESSING_TASK` the type of processing task the neural net is intended to perform (currently limited to `annotation`, `segmentation`, `classification`)\n",
    "    * `PRETRAIN_WEIGHTS_FILE` (_optional_) \tdetermines the location of the pre-trained weights file; if the file does not exist and is needed, \n",
    "    the training program will download it from a predefined URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing  30  lines from file  brats_seg/config/environment.json starting at line 0\n",
      "{\n",
      "    \"DATA_ROOT\": \"/workspace/data\",\n",
      "    \"DATASET_JSON\": \"/path/to/datalist.json\",\n",
      "    \"PROCESSING_TASK\": \"segmentation\",\n",
      "    \"MMAR_EVAL_OUTPUT_PATH\": \"eval\",\n",
      "    \"MMAR_CKPT_DIR\": \"models\",\n",
      "    \"MMAR_SUBMIT_DIR\": \"submit\",\n",
      "    \"DATA_LIST_KEY\": \"test\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "printFile(MMAR_ROOT+\"/config/environment.json\",0,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "2. `train.sh` and `train_finetune.sh` run the commands to train the neural network based on the `config_train.json` configuration; \n",
    "this shell script can be also used to override parameters in `config_train.json` using the `--set` argument (see `train_finetune.sh`)\n",
    "\n",
    "_Note_: The main difference between the two is that `train_finetune.sh` specifies a `ckpt` file, \n",
    "while `train.sh` does not since it is training from scratch.\n",
    "\n",
    "Let's take a look at `train.sh` by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing  30  lines from file  brats_seg/commands/train_finetune.sh starting at line 0\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "my_dir=\"$(dirname \"$0\")\"\n",
      ". $my_dir/set_env.sh\n",
      "\n",
      "echo \"MMAR_ROOT set to $MMAR_ROOT\"\n",
      "additional_options=\"$*\"\n",
      "\n",
      "# Data list containing all data\n",
      "CONFIG_FILE=config/config_train.json\n",
      "ENVIRONMENT_FILE=config/environment.json\n",
      "\n",
      "python -u  -m nvmidl.apps.train \\\n",
      "    -m $MMAR_ROOT \\\n",
      "    -c $CONFIG_FILE \\\n",
      "    -e $ENVIRONMENT_FILE \\\n",
      "    --set \\\n",
      "    DATASET_JSON=$MMAR_ROOT/config/seg_brats18_datalist_0.json \\\n",
      "    epochs=1250 \\\n",
      "    MMAR_CKPT=$MMAR_ROOT/models/model.ckpt \\\n",
      "    ${additional_options}\n"
     ]
    }
   ],
   "source": [
    "# printFile(MMAR_ROOT+\"/commands/train_W_Config.sh\",30,30)\n",
    "printFile(MMAR_ROOT+\"/commands/train_finetune.sh\",0,30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## config.json Main Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "`config_train.json` contains all the parameters necessary to define the neural network, \n",
    "how is it trained (training hyper-parameters, loss, etc.), \n",
    "pre- and post-transformation functions necessary to modify and/or augment the data before input to the neural net, etc. \n",
    "The complete documentation on the training configuration is laid out \n",
    "[here](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/appendix/configuration.html#training-configuration).\n",
    "The configuration file defines all training related parameters. \n",
    "This is were a researcher would spend most of their time.\n",
    "\n",
    "<br>![s](screenShots/MMARParts.png)<br> \n",
    "\n",
    "Lets take some time to examine each component of this configuration file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Global configurations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "2. Training config which includes:\n",
    "    1. Loss functions:\n",
    "    [Dice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=dice#module-ai4med.components.losses.dice)\n",
    "    , [CrossEntropy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=crossentropy#ai4med.components.losses.cross_entropy.CrossEntropy)\n",
    "    , [Focal](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=focal#module-ai4med.components.losses.focal)\n",
    "    , [FocalDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=focaldice#ai4med.components.losses.focal_dice.FocalDice) \n",
    "    , [CrossEntropyDice](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=crossentropydice#ai4med.components.losses.cross_entropy_dice.CrossEntropyDice) \n",
    "    , [BinaryClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=binaryclassificationloss#ai4med.components.losses.classification_loss.BinaryClassificationLoss)\n",
    "    , [MulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=multiclassclassificationloss#ai4med.components.losses.classification_loss.MulticlassClassificationLoss)\n",
    "    , [WeightedMulticlassClassificationLoss](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.losses.html?highlight=weightedmulticlassclassificationloss#ai4med.components.losses.classification_loss.WeightedMulticlassClassificationLoss)\n",
    "    2. Optimizer\n",
    "    [Momentum]()\n",
    "    , [Adam]()\n",
    "    , [NovaGrad]()\n",
    "    3. Network architecture\n",
    "    [SegAhnet]()\n",
    "    , [SegResnet]()\n",
    "    , [Unet]()\n",
    "    , [UnetParallel]()\n",
    "    , [DenseNet121]()\n",
    "    , [Alexnet]()\n",
    "    4. Learing rate Policy \n",
    "    [ReducePoly]()\n",
    "    , [DecayOnStep]()\n",
    "    , [ReduceCosine]()\n",
    "    , [ReduceOnPlateau]()\n",
    "    5. Image pipeline\n",
    "        1. Classification \n",
    "        , [ClassificationImagePipeline]()\n",
    "        , [ClassificationImagePipelineWithCache]()\n",
    "        , [ClassificationKerasImagePipeline]()\n",
    "        , [ClassificationKerasImagePipelineWithCache]()\n",
    "        2. Segmenatation \n",
    "        , [SegmentationImagePipeline]()\n",
    "        , [SegmentationImagePipelineWithCache]()\n",
    "        , [SegmentationKerasImagePipeline]()\n",
    "        , [SegmentationKerasImagePipelineWithCache]()    \n",
    "    4. Pretransforms\n",
    "        1. Loading transformations:\n",
    "            [LoadNifti](https://docs.nvidia.com/clara/tlt-m[i/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadnifti#ai4med.components.transforms.load_nifti.LoadNifti)\n",
    "            , [LoadPng](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadpng#ai4med.components.transforms.load_png.LoadPng)\n",
    "            , [ConvertToChannelsFirst](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttochannelsfirst#ai4med.components.transforms.convert_to_channels_first.ConvertToChannelsFirst)\n",
    "            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n",
    "            , [LoadJpg](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadjpg#ai4med.components.transforms.load_jpg.LoadJpg)\n",
    "        2. Resample Transformation\n",
    "            [RepeatChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=repeatchannel#ai4med.components.transforms.repeat_channel.RepeatChannel)\n",
    "            , [ScaleByFactor](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyfactor#ai4med.components.transforms.scale_by_factor.ScaleByFactor)\n",
    "            , [ScaleByResolution](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyresolution#ai4med.components.transforms.scale_by_resolution.ScaleByResolution)\n",
    "            , [ScaleBySpacing](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scalebyspacing#ai4med.components.transforms.scale_by_spacing.ScaleBySpacing)\n",
    "            , [ScaleToShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaletoshape#ai4med.components.transforms.scale_to_shape.ScaleToShape)\n",
    "            , [RestoreOriginalShape](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=restoreoriginalshape#ai4med.components.transforms.restore_original_shape.RestoreOriginalShape)\n",
    "            , [LoadImageMasksFromNumpy](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=loadimagemasksfromnumpy#ai4med.components.transforms.load_image_masks_from_numpy.LoadImageMasksFromNumpy)\n",
    "        3. Cropping transformations\n",
    "            [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n",
    "            , [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n",
    "            , [CropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropbyposnegratio#ai4med.components.transforms.crop_by_pos_neg_ratio.CropByPosNegRatio)\n",
    "            , [SymmetricPadderDiv](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=symmetricpadderdiv#ai4med.components.transforms.symmetric_padder_div.SymmetricPadderDiv)\n",
    "            , [FastCropByPosNegRatio](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastcropbyposnegratio#ai4med.components.transforms.fast_crop_by_pos_neg_ratio.FastCropByPosNegRatio)\n",
    "            , [CropByPosNegRatioLabelOnly](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropbyposnegratiolabelonly#ai4med.components.transforms.crop_by_pos_neg_ratio_label_only.CropByPosNegRatioLabelOnly)\n",
    "            , [CropForegroundObject](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropforegroundobject#ai4med.components.transforms.crop_foreground_object.CropForegroundObject)\n",
    "            , [CropSubVolumeCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropsubvolumecenter#ai4med.components.transforms.crop_sub_volume_center.CropSubVolumeCenter)\n",
    "            , [CropRandomSizeWithDisplacement](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=croprandomsizewithdisplacement#ai4med.components.transforms.crop_random_size_w_displacement.CropRandomSizeWithDisplacement)\n",
    "            , [CropFixedSizeRandomCenter](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=cropfixedsizerandomcenter#ai4med.components.transforms.crop_fixed_size_random_center.CropFixedSizeRandomCenter)\n",
    "        4. Deformable transformations\n",
    "            [FastPosNegRatioCropROI](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=fastposnegratiocroproi#ai4med.components.transforms.fast_pos_neg_ratio_crop_roi.FastPosNegRatioCropROI)\n",
    "        5. Intensity Transforms\n",
    "            [ScaleIntensityRange](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaleintensityrange#ai4med.components.transforms.scale_intensity_range.ScaleIntensityRange)\n",
    "            , [ScaleIntensityOscillation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=scaleintensityoscillation#ai4med.components.transforms.scale_intensity_oscillation.ScaleIntensityOscillation)\n",
    "            , [AddGaussianNoise](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=addgaussiannoise#ai4med.components.transforms.add_gaussian_noise.AddGaussianNoise)\n",
    "            , [NormalizeNonzeroIntensities](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=normalizenonzerointensities#ai4med.components.transforms.normalize_nonzero_intensities.NormalizeNonzeroIntensities)\n",
    "            , [CenterData](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=centerdata#ai4med.components.transforms.center_data.CenterData)\n",
    "            , [AdjustContrast](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=adjustcontrast#ai4med.components.transforms.adjust_contrast.AdjustContrast)\n",
    "            , [RandomGaussianSmooth]()\n",
    "            , [RandomMRBiasField]()\n",
    "        6. Augmentation Transforms\n",
    "            [RandomZoom](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomzoom#ai4med.components.transforms.random_zoom.RandomZoom)\n",
    "            , [RandomAxisFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomaxisflip#ai4med.components.transforms.random_axis_flip.RandomAxisFlip)\n",
    "            , [RandomSpatialFlip](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomspatialflip#ai4med.components.transforms.random_spatial_flip.RandomSpatialFlip)\n",
    "            , [RandomRotate2D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomrotate2d#ai4med.components.transforms.random_rotate_2d.RandomRotate2D)\n",
    "            , [RandomRotate3D](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=randomrotate3d#ai4med.components.transforms.random_rotate_3d.RandomRotate3D)\n",
    "        7. Special transforms \n",
    "            [AddExtremePointsChannel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=addextremepointschannel#ai4med.components.transforms.add_extreme_points_channel.AddExtremePointsChannel)\n",
    "            , [SplitAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitacrosschannels#ai4med.components.transforms.split_across_channels.SplitAcrossChannels)\n",
    "            , [SplitBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitbasedonlabel#ai4med.components.transforms.split_based_on_label.SplitBasedOnLabel)\n",
    "            , [ThresholdValues](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=thresholdvalues#ai4med.components.transforms.apply_threshold.ThresholdValues)\n",
    "            , [SplitBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=splitbasedonbratsclasses#ai4med.components.transforms.split_based_on_brats_classes.SplitBasedOnBratsClasses)\n",
    "            , [ConvertToMultiChannelBasedOnLabel](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttomultichannelbasedonlabel#ai4med.components.transforms.convert_to_multi_channel_based_on_label.ConvertToMultiChannelBasedOnLabel)\n",
    "            , [KeepLargestCC](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=keeplargestcc#ai4med.components.transforms.keep_largest_connected_component.KeepLargestCC)\n",
    "            , [CopyProperties](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=copyproperties#ai4med.components.transforms.copy_properties.CopyProperties)\n",
    "            , [ConvertToMultiChannelBasedOnBratsClasses](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=converttomultichannelbasedonbratsclasses#ai4med.components.transforms.convert_to_multi_channel_based_on_brats_classes.ConvertToMultiChannelBasedOnBratsClasses)\n",
    "            , [ArgmaxAcrossChannels](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/apidocs/ai4med/ai4med.components.transforms.html?highlight=argmaxacrosschannels#ai4med.components.transforms.argmax_across_channels.ArgmaxAcrossChannels)       \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "3. Validation config which includes:\n",
    "    1. Metric \n",
    "    2. pre-transforms. Since these transforms are usually a subset from the pre-transforms in the training section, \n",
    "    we can use the alias to point to these transforms by name as ` \"ref\": \"LoadNifti\"`. \n",
    "    In case we use 2 transforms with the same name as `ScaleByResolution` \n",
    "    we can give each an alias to refer to as `\"name\": \"ScaleByResolution#ScaleImg\"` \n",
    "    then refer to it in the validation section as `ScaleImg` \n",
    "    3. Image pipeline\n",
    "    4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing  13  lines from file  /claraDevDay/MMARs/GettingStarted//config/trn_base.json starting at line 120\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"validate\": {\n",
      "    \"metrics\": [\n",
      "      {\n",
      "        \"name\": \"ComputeAverageDice\",\n",
      "        \"args\": {\n",
      "          \"name\": \"mean_dice\",\n",
      "          \"is_key_metric\": true,\n",
      "          \"field\": \"model\",\n",
      "          \"label_field\": \"label\"\n",
      "        }\n",
      "      }\n",
      "showing  16  lines from file  /claraDevDay/MMARs/GettingStarted//config/trn_base.json starting at line 135\n",
      "    \"pre_transforms\": [\n",
      "       {\n",
      "         \"ref\": \"LoadNifti\"\n",
      "       },\n",
      "       {\n",
      "         \"ref\": \"ConvertToChannelsFirst\"\n",
      "       },\n",
      "       {\n",
      "         \"ref\": \"ScaleImg\"\n",
      "       },\n",
      "       {\n",
      "         \"ref\": \"ScaleLb\"\n",
      "       },\n",
      "       {\n",
      "         \"ref\": \"ScaleIntensityRange\"\n",
      "       }\n",
      "    ],\n",
      "showing  12  lines from file  /claraDevDay/MMARs/GettingStarted//config/trn_base.json starting at line 152\n",
      "    \"image_pipeline\": {\n",
      "      \"name\": \"SegmentationImagePipeline\",\n",
      "      \"args\": {\n",
      "        \"data_list_file_path\": \"{DATASET_JSON}\",\n",
      "        \"data_file_base_dir\": \"{DATA_ROOT}\",\n",
      "        \"data_list_key\": \"validation\",\n",
      "        \"output_crop_size\": [64, 64, 64],\n",
      "        \"output_batch_size\": 1,\n",
      "        \"num_workers\": 2,\n",
      "        \"prefetch_size\": 2\n",
      "      }\n",
      "    },\n",
      "    \"inferer\": {\n",
      "showing  10  lines from file  /claraDevDay/MMARs/GettingStarted//config/trn_base.json starting at line 164\n",
      "    \"inferer\": {\n",
      "      \"name\": \"TFScanWindowInferer\",\n",
      "      \"args\": {\n",
      "        \"roi_size\": [64, 64, 64]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "confFile=MMAR_ROOT+\"/config/trn_base.json\"\n",
    "printFile(confFile,120,13)\n",
    "printFile(confFile,135,16)\n",
    "printFile(confFile,152,12)\n",
    "printFile(confFile,164,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "## Start TensorBoard \n",
    "Before launching a training run or while the neural network is training, \n",
    "users can monitor the accuracy and other metrics using tensorboard in a side jupyter lab tab as shown below\n",
    " <br>![tb](screenShots/TensorBoard.png)<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lets start training\n",
    "Now that we have our training configuration, to start training simply run `train.sh` as below. \n",
    "Please keep in mind that we have setup a dummy dataset with one file to train a small neural network quickly (we only train for 2 epochs). \n",
    "Please see exercises on how to easily switch data and train a real segmentation network.\n",
    "\n",
    "**_Note:_** We have renamed `train.sh` to `train_W_Config.sh` as we modified it to accept parameters with the configuration to use       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "docker stop brats_seg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$DOCKER_NAME\" \"$DATASET_PREFIX\" \"$MMAR_ROOT\" \"$PRETRAIN_PATH\"\n",
    "docker run --runtime=nvidia --name brats_seg_train --rm -dt --shm-size=20g --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=6 \\\n",
    "    -v $PWD:/mmar \\\n",
    "    -v $2:/workspace/data \\\n",
    "    -v $4:/workspace/pretrain \\\n",
    "    $1 \\\n",
    "    bash -c \"bash /mmar/$3/commands/train.sh 2>&1 | tee /mmar/$3/logs/train_log.tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now let us navigate the `models` directory, which would includes out models and the tensorboard files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 125148\n",
      "drwxr-xr-x 2 root    root        4096 Oct 16 19:10 .\n",
      "drwxrwxr-x 8 liuyuan liuyuan     4096 Oct 16 16:47 ..\n",
      "-rw-r--r-- 1 root    root         130 Oct 16 19:10 checkpoint\n",
      "-rw-r--r-- 1 root    root     4936181 Oct 15 22:13 events.out.tfevents.1602771069.866ce21f8596\n",
      "-rw-r--r-- 1 root    root     5110938 Oct 16 19:10 events.out.tfevents.1602771277.11cf40bccf9c\n",
      "-rw-r--r-- 1 root    root    56426740 Oct 16 16:27 model.ckpt.data-00000-of-00001\n",
      "-rw-r--r-- 1 root    root       10048 Oct 16 16:27 model.ckpt.index\n",
      "-rw-r--r-- 1 root    root     2599518 Oct 16 16:27 model.ckpt.meta\n",
      "-rw-r--r-- 1 root    root    56426740 Oct 16 19:10 model_final.ckpt.data-00000-of-00001\n",
      "-rw-r--r-- 1 root    root       10048 Oct 16 19:10 model_final.ckpt.index\n",
      "-rw-r--r-- 1 root    root     2599518 Oct 16 19:10 model_final.ckpt.meta\n"
     ]
    }
   ],
   "source": [
    "! ls -la $MMAR_ROOT/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "## Export Model\n",
    "\n",
    "To export the model we simply run `export.sh` which will: \n",
    "- Remove back propagation information from checkpoint files\n",
    "- Generate two frozen graphs in the models folder\n",
    "This optimized model will be used by Triton Inference server in the Clara Deploy SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMAR_ROOT set to /home/liuyuan/shu_codes/clara_fl_brats/brats_seg/commands/..\n",
      "/home/liuyuan/programs/miniconda3/envs/pt/bin/python: Error while finding module specification for 'nvmidl.apps.export' (ModuleNotFoundError: No module named 'nvmidl')\n"
     ]
    }
   ],
   "source": [
    "! $MMAR_ROOT/commands/export.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "lets check out what was created in the folder. \n",
    "after running cell below you should see:\n",
    "1. Frozen File Generated: /claraDevDay/MMARs/GettingStarted/commands/../models/trn_base/model.fzn.pb\n",
    "2. TRT File Generated: /claraDevDay/MMARs/GettingStarted/commands/../models/trn_base/model.trt.pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 125148\n",
      "drwxr-xr-x 2 root    root        4096 Oct 16 19:10 .\n",
      "drwxrwxr-x 8 liuyuan liuyuan     4096 Oct 16 16:47 ..\n",
      "-rw-r--r-- 1 root    root         130 Oct 16 19:10 checkpoint\n",
      "-rw-r--r-- 1 root    root     4936181 Oct 15 22:13 events.out.tfevents.1602771069.866ce21f8596\n",
      "-rw-r--r-- 1 root    root     5110938 Oct 16 19:10 events.out.tfevents.1602771277.11cf40bccf9c\n",
      "-rw-r--r-- 1 root    root    56426740 Oct 16 16:27 model.ckpt.data-00000-of-00001\n",
      "-rw-r--r-- 1 root    root       10048 Oct 16 16:27 model.ckpt.index\n",
      "-rw-r--r-- 1 root    root     2599518 Oct 16 16:27 model.ckpt.meta\n",
      "-rw-r--r-- 1 root    root    56426740 Oct 16 19:10 model_final.ckpt.data-00000-of-00001\n",
      "-rw-r--r-- 1 root    root       10048 Oct 16 19:10 model_final.ckpt.index\n",
      "-rw-r--r-- 1 root    root     2599518 Oct 16 19:10 model_final.ckpt.meta\n"
     ]
    }
   ],
   "source": [
    "!ls -la $MMAR_ROOT/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "## Evaluate and Prediction \n",
    "Now that we have trained our model we would like to run evaluation to get some statistics and also do inference to see the resulting prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 1. Evaluate \n",
    "To run evaluation on your validation dataset you should run `validate.sh`. \n",
    "This will run evaluation on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the [environment.json](config/environment.json) \n",
    "file (default is eval folder). \n",
    "This evaluation would give min, max, mean of the metric as specified in the config_validation file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMAR_ROOT set to /workspace/clara_seg_ct_brats/commands/..\n",
      "2020-09-22 01:42:13.365580: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "[[35311,1],0]: A high-performance Open MPI point-to-point messaging module\n",
      "was unable to find any relevant network interfaces:\n",
      "\n",
      "Module: OpenFabrics (openib)\n",
      "  Host: b90125841dd2\n",
      "\n",
      "Another transport will be used instead, although this may result in\n",
      "lower performance.\n",
      "\n",
      "NOTE: You can disable this warning by setting the MCA parameter\n",
      "btl_base_warn_component_unused to 0.\n",
      "--------------------------------------------------------------------------\n",
      "Using TensorFlow backend.\n",
      "2020-09-22 01:42:15,446 - nvmidl.utils.train_conf - INFO - Automatic Mixed Precision status: Disabled\n",
      "Previously evaluated: 0 ; To be evaluated: 50\n",
      "2020-09-22 01:42:15.759927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\n",
      "2020-09-22 01:42:15.766242: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x258f7b0 executing computations on platform Host. Devices:\n",
      "2020-09-22 01:42:15.766296: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-09-22 01:42:15.771602: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2020-09-22 01:42:15.954908: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x26100a0 executing computations on platform CUDA. Devices:\n",
      "2020-09-22 01:42:15.954980: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2020-09-22 01:42:15.957969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545\n",
      "pciBusID: 0000:b2:00.0\n",
      "2020-09-22 01:42:15.958046: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-09-22 01:42:15.958222: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\n",
      "2020-09-22 01:42:15.958297: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\n",
      "2020-09-22 01:42:15.958370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\n",
      "2020-09-22 01:42:15.962245: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-09-22 01:42:15.964269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-09-22 01:42:15.964404: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-09-22 01:42:15.969087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2020-09-22 01:42:15.969159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-09-22 01:42:16.544329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-09-22 01:42:16.544371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2020-09-22 01:42:16.544379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2020-09-22 01:42:16.546985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10156 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5)\n",
      "2020-09-22 01:42:19.045291: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "Batch 1 / 50: 8.67s; pre-process: 1.40s; infer: 6.48s; post-process: 0.79s\n",
      "Batch 2 / 50: 6.22s; pre-process: 1.21s; infer: 4.22s; post-process: 0.78s\n",
      "Batch 3 / 50: 6.24s; pre-process: 1.18s; infer: 4.27s; post-process: 0.79s\n",
      "Batch 4 / 50: 6.23s; pre-process: 1.22s; infer: 4.24s; post-process: 0.77s\n",
      "Batch 5 / 50: 6.23s; pre-process: 1.20s; infer: 4.26s; post-process: 0.77s\n",
      "Batch 6 / 50: 6.19s; pre-process: 1.17s; infer: 4.24s; post-process: 0.78s\n",
      "Batch 7 / 50: 6.25s; pre-process: 1.18s; infer: 4.27s; post-process: 0.79s\n",
      "Batch 8 / 50: 6.26s; pre-process: 1.23s; infer: 4.26s; post-process: 0.78s\n",
      "Batch 9 / 50: 6.23s; pre-process: 1.19s; infer: 4.24s; post-process: 0.79s\n",
      "Batch 10 / 50: 6.62s; pre-process: 1.21s; infer: 4.66s; post-process: 0.75s\n",
      "Batch 11 / 50: 6.16s; pre-process: 1.15s; infer: 4.24s; post-process: 0.77s\n",
      "Batch 12 / 50: 6.18s; pre-process: 1.15s; infer: 4.24s; post-process: 0.79s\n",
      "Batch 13 / 50: 6.39s; pre-process: 1.22s; infer: 4.39s; post-process: 0.78s\n",
      "Batch 14 / 50: 6.28s; pre-process: 1.20s; infer: 4.31s; post-process: 0.78s\n",
      "Batch 15 / 50: 6.18s; pre-process: 1.15s; infer: 4.24s; post-process: 0.79s\n",
      "Batch 16 / 50: 6.19s; pre-process: 1.17s; infer: 4.24s; post-process: 0.77s\n",
      "Batch 17 / 50: 6.18s; pre-process: 1.18s; infer: 4.24s; post-process: 0.77s\n",
      "Batch 18 / 50: 6.22s; pre-process: 1.17s; infer: 4.27s; post-process: 0.78s\n",
      "Batch 19 / 50: 6.24s; pre-process: 1.19s; infer: 4.28s; post-process: 0.78s\n",
      "Batch 20 / 50: 6.21s; pre-process: 1.18s; infer: 4.25s; post-process: 0.78s\n",
      "Batch 21 / 50: 6.22s; pre-process: 1.20s; infer: 4.24s; post-process: 0.79s\n",
      "Batch 22 / 50: 6.20s; pre-process: 1.19s; infer: 4.24s; post-process: 0.76s\n",
      "Batch 23 / 50: 6.20s; pre-process: 1.17s; infer: 4.25s; post-process: 0.78s\n",
      "Batch 24 / 50: 6.22s; pre-process: 1.19s; infer: 4.25s; post-process: 0.78s\n",
      "Batch 25 / 50: 6.16s; pre-process: 1.12s; infer: 4.27s; post-process: 0.77s\n",
      "Batch 26 / 50: 6.19s; pre-process: 1.15s; infer: 4.27s; post-process: 0.77s\n",
      "Batch 27 / 50: 6.19s; pre-process: 1.14s; infer: 4.26s; post-process: 0.79s\n",
      "Batch 28 / 50: 6.24s; pre-process: 1.19s; infer: 4.28s; post-process: 0.77s\n",
      "Batch 29 / 50: 6.22s; pre-process: 1.17s; infer: 4.28s; post-process: 0.77s\n",
      "Batch 30 / 50: 6.19s; pre-process: 1.16s; infer: 4.26s; post-process: 0.77s\n",
      "Batch 31 / 50: 6.19s; pre-process: 1.16s; infer: 4.26s; post-process: 0.77s\n",
      "Batch 32 / 50: 6.22s; pre-process: 1.19s; infer: 4.25s; post-process: 0.77s\n",
      "Batch 33 / 50: 6.18s; pre-process: 1.16s; infer: 4.25s; post-process: 0.77s\n",
      "Batch 34 / 50: 6.23s; pre-process: 1.19s; infer: 4.27s; post-process: 0.78s\n",
      "Batch 35 / 50: 6.23s; pre-process: 1.18s; infer: 4.27s; post-process: 0.78s\n",
      "Batch 36 / 50: 6.19s; pre-process: 1.15s; infer: 4.28s; post-process: 0.77s\n",
      "Batch 37 / 50: 6.21s; pre-process: 1.16s; infer: 4.26s; post-process: 0.78s\n",
      "Batch 38 / 50: 6.22s; pre-process: 1.17s; infer: 4.27s; post-process: 0.78s\n",
      "Batch 39 / 50: 6.23s; pre-process: 1.18s; infer: 4.27s; post-process: 0.77s\n",
      "Batch 40 / 50: 6.23s; pre-process: 1.17s; infer: 4.28s; post-process: 0.78s\n",
      "Batch 41 / 50: 6.21s; pre-process: 1.17s; infer: 4.26s; post-process: 0.78s\n",
      "Batch 42 / 50: 6.18s; pre-process: 1.15s; infer: 4.26s; post-process: 0.78s\n",
      "Batch 43 / 50: 6.25s; pre-process: 1.19s; infer: 4.28s; post-process: 0.77s\n",
      "Batch 44 / 50: 6.34s; pre-process: 1.19s; infer: 4.38s; post-process: 0.77s\n",
      "Batch 45 / 50: 6.21s; pre-process: 1.18s; infer: 4.26s; post-process: 0.76s\n",
      "Batch 46 / 50: 6.19s; pre-process: 1.15s; infer: 4.27s; post-process: 0.76s\n",
      "Batch 47 / 50: 6.22s; pre-process: 1.16s; infer: 4.28s; post-process: 0.78s\n",
      "Batch 48 / 50: 6.23s; pre-process: 1.16s; infer: 4.28s; post-process: 0.78s\n",
      "Batch 49 / 50: 6.26s; pre-process: 1.22s; infer: 4.26s; post-process: 0.78s\n",
      "Batch 50 / 50: 6.18s; pre-process: 1.14s; infer: 4.26s; post-process: 0.78s\n",
      "\n",
      "\n",
      "Total Inference Time: 215.85340642929077s\n",
      "2020-09-22 01:47:31,310 - nvmidl.utils.train_conf - INFO - Total Evaluation Time 316.39779829978943\n"
     ]
    }
   ],
   "source": [
    "! $MMAR_ROOT/commands/validate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now let us see the results in the folder by running cells below. \n",
    "You should see statistics and dice per file in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92\n",
      "drwxrwxr-x 2 1001 1001  4096 Sep 22 01:47 .\n",
      "drwxrwxr-x 8 1001 1001  4096 Sep 21 12:56 ..\n",
      "-rw-rw-r-- 1 1001 1001     0 Jun 30 15:24 _gitignore\n",
      "-rw-r--r-- 1 root root 21593 Sep 22 01:47 mean_dice_ET_raw_results.txt\n",
      "-rw-r--r-- 1 root root   148 Sep 22 01:47 mean_dice_ET_summary_results.txt\n",
      "-rw-r--r-- 1 root root 21597 Sep 22 01:47 mean_dice_TC_raw_results.txt\n",
      "-rw-r--r-- 1 root root   148 Sep 22 01:47 mean_dice_TC_summary_results.txt\n",
      "-rw-r--r-- 1 root root 21591 Sep 22 01:47 mean_dice_WT_raw_results.txt\n",
      "-rw-r--r-- 1 root root   148 Sep 22 01:47 mean_dice_WT_summary_results.txt\n"
     ]
    }
   ],
   "source": [
    "! ls -la $MMAR_ROOT/eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_dice_ET (statistics of 50 valid cases):\n",
      "    mean  median     max     min   90percent   std\n",
      "   0.874   0.891   0.960   0.520   0.811     0.075\n",
      "\n",
      "mean_dice_WT (statistics of 50 valid cases):\n",
      "    mean  median     max     min   90percent   std\n",
      "   0.918   0.931   0.968   0.664   0.886     0.058\n",
      "\n",
      "mean_dice_TC (statistics of 50 valid cases):\n",
      "    mean  median     max     min   90percent   std\n",
      "   0.906   0.944   0.974   0.494   0.827     0.102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# statistic summary\n",
    "!cat $MMAR_ROOT/eval/mean_dice_ET_summary_results.txt\n",
    "!cat $MMAR_ROOT/eval/mean_dice_WT_summary_results.txt\n",
    "!cat $MMAR_ROOT/eval/mean_dice_TC_summary_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGX_1/BraTS19_CBICA_BGX_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGX_1/BraTS19_CBICA_BGX_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGX_1/BraTS19_CBICA_BGX_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGX_1/BraTS19_CBICA_BGX_1_flair.nii.gz\t0.886624675805854\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYG_1/BraTS19_CBICA_AYG_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYG_1/BraTS19_CBICA_AYG_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYG_1/BraTS19_CBICA_AYG_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYG_1/BraTS19_CBICA_AYG_1_flair.nii.gz\t0.8989393296563428\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCF_1/BraTS19_CBICA_BCF_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCF_1/BraTS19_CBICA_BCF_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCF_1/BraTS19_CBICA_BCF_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCF_1/BraTS19_CBICA_BCF_1_flair.nii.gz\t0.8456456456456456\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVB_1/BraTS19_CBICA_AVB_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVB_1/BraTS19_CBICA_AVB_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVB_1/BraTS19_CBICA_AVB_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVB_1/BraTS19_CBICA_AVB_1_flair.nii.gz\t0.8432601880877743\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGW_1/BraTS19_CBICA_BGW_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGW_1/BraTS19_CBICA_BGW_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGW_1/BraTS19_CBICA_BGW_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGW_1/BraTS19_CBICA_BGW_1_flair.nii.gz\t0.7984290802813042\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BJY_1/BraTS19_CBICA_BJY_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BJY_1/BraTS19_CBICA_BJY_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BJY_1/BraTS19_CBICA_BJY_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BJY_1/BraTS19_CBICA_BJY_1_flair.nii.gz\t0.8708832445477823\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOC_1/BraTS19_CBICA_AOC_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOC_1/BraTS19_CBICA_AOC_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOC_1/BraTS19_CBICA_AOC_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOC_1/BraTS19_CBICA_AOC_1_flair.nii.gz\t0.8731786292498651\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGG_1/BraTS19_CBICA_BGG_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGG_1/BraTS19_CBICA_BGG_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGG_1/BraTS19_CBICA_BGG_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGG_1/BraTS19_CBICA_BGG_1_flair.nii.gz\t0.9247095543610196\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGT_1/BraTS19_CBICA_BGT_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGT_1/BraTS19_CBICA_BGT_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGT_1/BraTS19_CBICA_BGT_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGT_1/BraTS19_CBICA_BGT_1_flair.nii.gz\t0.8241888541554382\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_11964_1/BraTS19_TMC_11964_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_11964_1/BraTS19_TMC_11964_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_11964_1/BraTS19_TMC_11964_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_11964_1/BraTS19_TMC_11964_1_flair.nii.gz\t0.9238554894334848\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_12866_1/BraTS19_TMC_12866_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_12866_1/BraTS19_TMC_12866_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_12866_1/BraTS19_TMC_12866_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_12866_1/BraTS19_TMC_12866_1_flair.nii.gz\t0.9598103233830846\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_21360_1/BraTS19_TMC_21360_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_21360_1/BraTS19_TMC_21360_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_21360_1/BraTS19_TMC_21360_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_21360_1/BraTS19_TMC_21360_1_flair.nii.gz\t0.9117068589754947\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGE_1/BraTS19_CBICA_BGE_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGE_1/BraTS19_CBICA_BGE_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGE_1/BraTS19_CBICA_BGE_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGE_1/BraTS19_CBICA_BGE_1_flair.nii.gz\t0.8548249197271673\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ATN_1/BraTS19_CBICA_ATN_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ATN_1/BraTS19_CBICA_ATN_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ATN_1/BraTS19_CBICA_ATN_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ATN_1/BraTS19_CBICA_ATN_1_flair.nii.gz\t0.8126253182238042\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_27374_1/BraTS19_TMC_27374_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_27374_1/BraTS19_TMC_27374_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_27374_1/BraTS19_TMC_27374_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_27374_1/BraTS19_TMC_27374_1_flair.nii.gz\t0.925006436426496\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHZ_1/BraTS19_CBICA_BHZ_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHZ_1/BraTS19_CBICA_BHZ_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHZ_1/BraTS19_CBICA_BHZ_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHZ_1/BraTS19_CBICA_BHZ_1_flair.nii.gz\t0.8844106463878327\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BNR_1/BraTS19_CBICA_BNR_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BNR_1/BraTS19_CBICA_BNR_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BNR_1/BraTS19_CBICA_BNR_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BNR_1/BraTS19_CBICA_BNR_1_flair.nii.gz\t0.693374160074849\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_15477_1/BraTS19_TMC_15477_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_15477_1/BraTS19_TMC_15477_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_15477_1/BraTS19_TMC_15477_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_15477_1/BraTS19_TMC_15477_1_flair.nii.gz\t0.8943872861505618\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BLJ_1/BraTS19_CBICA_BLJ_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BLJ_1/BraTS19_CBICA_BLJ_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BLJ_1/BraTS19_CBICA_BLJ_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BLJ_1/BraTS19_CBICA_BLJ_1_flair.nii.gz\t0.520297699594046\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOS_1/BraTS19_CBICA_AOS_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOS_1/BraTS19_CBICA_AOS_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOS_1/BraTS19_CBICA_AOS_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AOS_1/BraTS19_CBICA_AOS_1_flair.nii.gz\t0.9283129995793017\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYC_1/BraTS19_CBICA_AYC_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYC_1/BraTS19_CBICA_AYC_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYC_1/BraTS19_CBICA_AYC_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AYC_1/BraTS19_CBICA_AYC_1_flair.nii.gz\t0.9097272314391814\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGN_1/BraTS19_CBICA_BGN_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGN_1/BraTS19_CBICA_BGN_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGN_1/BraTS19_CBICA_BGN_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGN_1/BraTS19_CBICA_BGN_1_flair.nii.gz\t0.8804930078217587\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_APK_1/BraTS19_CBICA_APK_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_APK_1/BraTS19_CBICA_APK_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_APK_1/BraTS19_CBICA_APK_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_APK_1/BraTS19_CBICA_APK_1_flair.nii.gz\t0.9045084293362378\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGO_1/BraTS19_CBICA_BGO_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGO_1/BraTS19_CBICA_BGO_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGO_1/BraTS19_CBICA_BGO_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGO_1/BraTS19_CBICA_BGO_1_flair.nii.gz\t0.8959818688323339\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06290_1/BraTS19_TMC_06290_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06290_1/BraTS19_TMC_06290_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06290_1/BraTS19_TMC_06290_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06290_1/BraTS19_TMC_06290_1_flair.nii.gz\t0.9537871524448706\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVF_1/BraTS19_CBICA_AVF_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVF_1/BraTS19_CBICA_AVF_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVF_1/BraTS19_CBICA_AVF_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVF_1/BraTS19_CBICA_AVF_1_flair.nii.gz\t0.8386818734512321\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BIC_1/BraTS19_CBICA_BIC_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BIC_1/BraTS19_CBICA_BIC_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BIC_1/BraTS19_CBICA_BIC_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BIC_1/BraTS19_CBICA_BIC_1_flair.nii.gz\t0.892417841570384\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_30014_1/BraTS19_TMC_30014_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_30014_1/BraTS19_TMC_30014_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_30014_1/BraTS19_TMC_30014_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_30014_1/BraTS19_TMC_30014_1_flair.nii.gz\t0.9145690885866706\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BKV_1/BraTS19_CBICA_BKV_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BKV_1/BraTS19_CBICA_BKV_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BKV_1/BraTS19_CBICA_BKV_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BKV_1/BraTS19_CBICA_BKV_1_flair.nii.gz\t0.9491891891891892\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASF_1/BraTS19_CBICA_ASF_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASF_1/BraTS19_CBICA_ASF_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASF_1/BraTS19_CBICA_ASF_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASF_1/BraTS19_CBICA_ASF_1_flair.nii.gz\t0.9293265383481447\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUX_1/BraTS19_CBICA_AUX_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUX_1/BraTS19_CBICA_AUX_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUX_1/BraTS19_CBICA_AUX_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUX_1/BraTS19_CBICA_AUX_1_flair.nii.gz\t0.8588021044111696\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ANV_1/BraTS19_CBICA_ANV_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ANV_1/BraTS19_CBICA_ANV_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ANV_1/BraTS19_CBICA_ANV_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ANV_1/BraTS19_CBICA_ANV_1_flair.nii.gz\t0.9539123479603938\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWV_1/BraTS19_CBICA_AWV_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWV_1/BraTS19_CBICA_AWV_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWV_1/BraTS19_CBICA_AWV_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWV_1/BraTS19_CBICA_AWV_1_flair.nii.gz\t0.7567691601652135\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BDK_1/BraTS19_CBICA_BDK_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BDK_1/BraTS19_CBICA_BDK_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BDK_1/BraTS19_CBICA_BDK_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BDK_1/BraTS19_CBICA_BDK_1_flair.nii.gz\t0.9318741055709417\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVT_1/BraTS19_CBICA_AVT_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVT_1/BraTS19_CBICA_AVT_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVT_1/BraTS19_CBICA_AVT_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AVT_1/BraTS19_CBICA_AVT_1_flair.nii.gz\t0.8493052827088616\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAN_1/BraTS19_CBICA_BAN_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAN_1/BraTS19_CBICA_BAN_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAN_1/BraTS19_CBICA_BAN_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAN_1/BraTS19_CBICA_BAN_1_flair.nii.gz\t0.891996720870193\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06643_1/BraTS19_TMC_06643_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06643_1/BraTS19_TMC_06643_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06643_1/BraTS19_TMC_06643_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TMC_06643_1/BraTS19_TMC_06643_1_flair.nii.gz\t0.8903691368304191\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASR_1/BraTS19_CBICA_ASR_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASR_1/BraTS19_CBICA_ASR_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASR_1/BraTS19_CBICA_ASR_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_ASR_1/BraTS19_CBICA_ASR_1_flair.nii.gz\t0.8581851345189239\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAX_1/BraTS19_CBICA_BAX_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAX_1/BraTS19_CBICA_BAX_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAX_1/BraTS19_CBICA_BAX_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAX_1/BraTS19_CBICA_BAX_1_flair.nii.gz\t0.9225798276735935\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BBG_1/BraTS19_CBICA_BBG_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BBG_1/BraTS19_CBICA_BBG_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BBG_1/BraTS19_CBICA_BBG_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BBG_1/BraTS19_CBICA_BBG_1_flair.nii.gz\t0.8967165947363968\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BEM_1/BraTS19_CBICA_BEM_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BEM_1/BraTS19_CBICA_BEM_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BEM_1/BraTS19_CBICA_BEM_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BEM_1/BraTS19_CBICA_BEM_1_flair.nii.gz\t0.719645712879813\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUA_1/BraTS19_CBICA_AUA_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUA_1/BraTS19_CBICA_AUA_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUA_1/BraTS19_CBICA_AUA_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUA_1/BraTS19_CBICA_AUA_1_flair.nii.gz\t0.8496472568013635\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGR_1/BraTS19_CBICA_BGR_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGR_1/BraTS19_CBICA_BGR_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGR_1/BraTS19_CBICA_BGR_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BGR_1/BraTS19_CBICA_BGR_1_flair.nii.gz\t0.9145112826253461\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWX_1/BraTS19_CBICA_AWX_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWX_1/BraTS19_CBICA_AWX_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWX_1/BraTS19_CBICA_AWX_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AWX_1/BraTS19_CBICA_AWX_1_flair.nii.gz\t0.8817856309933738\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHV_1/BraTS19_CBICA_BHV_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHV_1/BraTS19_CBICA_BHV_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHV_1/BraTS19_CBICA_BHV_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHV_1/BraTS19_CBICA_BHV_1_flair.nii.gz\t0.8700324725529612\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHQ_1/BraTS19_CBICA_BHQ_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHQ_1/BraTS19_CBICA_BHQ_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHQ_1/BraTS19_CBICA_BHQ_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BHQ_1/BraTS19_CBICA_BHQ_1_flair.nii.gz\t0.8974467484835661\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAP_1/BraTS19_CBICA_BAP_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAP_1/BraTS19_CBICA_BAP_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAP_1/BraTS19_CBICA_BAP_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BAP_1/BraTS19_CBICA_BAP_1_flair.nii.gz\t0.9463148316651502\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUW_1/BraTS19_CBICA_AUW_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUW_1/BraTS19_CBICA_AUW_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUW_1/BraTS19_CBICA_AUW_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_AUW_1/BraTS19_CBICA_AUW_1_flair.nii.gz\t0.8751969058565885\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCL_1/BraTS19_CBICA_BCL_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCL_1/BraTS19_CBICA_BCL_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCL_1/BraTS19_CBICA_BCL_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_CBICA_BCL_1/BraTS19_CBICA_BCL_1_flair.nii.gz\t0.8618695975728433\n",
      "/workspace/data/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS19_TMC_09043_1_t1ce.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS19_TMC_09043_1_t1.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS19_TMC_09043_1_t2.nii.gz,/workspace/data/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TMC_09043_1/BraTS19_TMC_09043_1_flair.nii.gz\t0.9499386420562476\n"
     ]
    }
   ],
   "source": [
    "!cat $MMAR_ROOT/eval/mean_dice_ET_raw_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 2. Predict\n",
    "\n",
    "To run inference on validation dataset or test dataset you should run `infer.sh`. \n",
    "This will run prediction on the validation dataset and place it in the `MMAR_EVAL_OUTPUT_PATH` as configured in the \n",
    "[environment.json](config/environment.json) file (default is eval folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "my_dir=\"$(dirname \"$0\")\"\n",
      ". $my_dir/set_env.sh\n",
      "\n",
      "echo \"MMAR_ROOT set to $MMAR_ROOT\"\n",
      "\n",
      "# Data list containing all data\n",
      "CONFIG_FILE=config/config_validation.json\n",
      "ENVIRONMENT_FILE=config/environment.json\n",
      "\n",
      "python -u  -m nvmidl.apps.evaluate \\\n",
      "    -m $MMAR_ROOT \\\n",
      "    -c $CONFIG_FILE \\\n",
      "    -e $ENVIRONMENT_FILE \\\n",
      "    --set \\\n",
      "    DATASET_JSON=$MMAR_ROOT/config/brats_2020_validation_submit.json \\\n",
      "    output_infer_result=true \\\n",
      "    do_validation=false \\\n",
      "    MMAR_CKPT_DIR=${MMAR_ROOT}/models \\\n",
      "    MMAR_EVAL_OUTPUT_PATH=${MMAR_ROOT}/eval \\\n",
      "    DATA_LIST_KEY=test\n"
     ]
    }
   ],
   "source": [
    "!cat $MMAR_ROOT/commands/infer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765af15c01d41ae80f90890e4e51fb6f9c188f04fd3e6c5e01282e480b072239\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$DOCKER_NAME\" \"$DATASET_PREFIX\" \"$MMAR_ROOT\" \"$PRETRAIN_PATH\"\n",
    "docker run --runtime=nvidia --name brats_seg_infer --rm -dt --shm-size=20g --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES='7' \\\n",
    "    -v $PWD:/mmar \\\n",
    "    -v $2:/workspace/data \\\n",
    "    -v $4:/workspace/pretrain \\\n",
    "    $1 \\\n",
    "    bash -c \"bash /mmar/$3/commands/infer.sh 2>&1 | tee /mmar/$3/logs/infer.tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $MMAR_ROOT/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Now lets see results in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84\n",
      "drwxr-xr-x 21 root    root    4096 Oct 18 16:05 .\n",
      "drwxrwxr-x  9 liuyuan liuyuan 4096 Oct 18 16:03 ..\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_001_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_002_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_003_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_004_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_005_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_006_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_007_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_008_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:04 BraTS20_Validation_009_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_010_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_011_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_012_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_013_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_014_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_015_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_016_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_017_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_018_t1ce\n",
      "drwxr-xr-x  2 root    root    4096 Oct 18 16:05 BraTS20_Validation_019_t1ce\n"
     ]
    }
   ],
   "source": [
    "!ls -la $MMAR_ROOT/eval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -la $MMAR_ROOT/eval/spleen_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Multi-GPU Training\n",
    "Clara train aims to simplify scaling and the utilization of all available gpu resources. \n",
    "Using the same config we already used for train we can simply invoke `train_2gpu.sh` to train on multiple gpus. \n",
    "We use MPI and Horovod to speed up training and passing weights between GPUs as shown below\n",
    "<br>![tb](screenShots/MultiGPU.png)<br> \n",
    "\n",
    "Let us examine the `train_2gpu.sh` script by running cell below. \n",
    "You can see we are changing the learning rate as the batch size has doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "printFile(MMAR_ROOT+\"/commands/train_2gpu.sh\",0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "Lets give it a try and run cell below to train on 2 gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! $MMAR_ROOT/commands/train_2gpu.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "\n",
    "\n",
    "# Exercise:\n",
    "Now that you are familiar with the Clara Train SDK, you can try to: \n",
    "1. Train on a full spleen dataset; to do this you could:\n",
    "    1. Download the spleen dataset using the [download](download) Notebook\n",
    "    2. Switch the dataset file in the [environment.json](config/environment.json)\n",
    "    3. rerun `train.sh`\n",
    "2. Explore different model architectures, losses, transformations by modifying or creating a new config file and running training\n",
    "3. Experiment with multi-GPU training by changing the number of gpus to train on from 2 to 3 or 4. \n",
    "You can edit [train_2gpu.sh](commands/train_2gpu.sh) then rerun the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
